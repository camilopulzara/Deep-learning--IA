{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\reto_ia\\ia\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\reto_ia\\ia\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# se cargan todas las librerias\n",
    "# Este notebook se corrio en python 3.9.13\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import spacy\n",
    "import re\n",
    "import unicodedata\n",
    "import inflect\n",
    "import keras \n",
    "import tensorflow \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datasets import Dataset, Features, ClassLabel, Value, DatasetDict, Sequence \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se cargan los datos\n",
    "train = pd.read_csv('train.csv')\n",
    "test_q = pd.read_csv('test_queries.csv')\n",
    "test_d = pd.read_csv('test_documents.csv')\n",
    "\n",
    "\n",
    "# Se crean diferentes funciones para la limpieza de texto.\n",
    "def quitar_tildes(texto):\n",
    "    \"\"\"\n",
    "    Elimina los tildes de una cadena de texto.\n",
    "\n",
    "    Parámetros:\n",
    "    - texto: Cadena de texto con tildes.\n",
    "\n",
    "    Retorna:\n",
    "    Una nueva cadena de texto sin tildes.\n",
    "    \"\"\"\n",
    "    texto_sin_tildes = unidecode(texto)\n",
    "    return texto_sin_tildes\n",
    "\n",
    "\n",
    "# Cargar el modelo de Spacy para inglés\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Funcion para quitar caracteres especiales de un texto utilizando expresiones regulares\n",
    "def remove_special_characters(text):\n",
    "    # Aplicar el modelo de Spacy para dividir el texto en tokens\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Reconstruir el texto eliminando caracteres especiales\n",
    "    cleaned_text = ' '.join([token.text for token in doc if not re.search('[^A-Za-z0-9]+', token.text)])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_hashtag_and_url(words):\n",
    "    # Eliminar URLs\n",
    "    words = re.sub(r'http\\S+|www.\\S+', '', words)\n",
    "    # Eliminar hashtags\n",
    "    words = re.sub(r'#\\S+', '', words)\n",
    "    # Eliminar usuarios @\n",
    "    words = re.sub(r'@\\S+', '', words)\n",
    "    # Elimina palabras con mas de aa\n",
    "    words =re.sub(r'\\b\\w*aa\\w*\\b','',words)\n",
    "    # Elimina palabras con mas de hh\n",
    "    words =re.sub(r'\\b\\w*hh\\w*\\b','',words)\n",
    "    # Elimina palabras con mas un ja r'mm+'\n",
    "    words =re.sub(r'(ja){2,}','',words)\n",
    "    # Elimina palabras con mas un mm\n",
    "    words =re.sub(r'mm+','',words)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"\n",
    "    Esta función toma una lista de palabras y elimina los caracteres no ASCII de cada palabra.\n",
    "    \n",
    "    Parámetros:\n",
    "    - words: Una lista de palabras que se van a limpiar.\n",
    "    \n",
    "    Retorna:\n",
    "    - Una cadena de texto que contiene las palabras limpias, sin caracteres no ASCII.\n",
    "    \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_word = new_word.replace('_', '')\n",
    "        new_word = new_word.replace('\\n', '')\n",
    "        new_words.append(new_word)\n",
    "    return str.join('', new_words)\n",
    "\n",
    "\n",
    "def reduce_repeated_characters(word, max_repeats=2):\n",
    "    # Utilizar una expresión regular para encontrar repeticiones excesivas\n",
    "    pattern = re.compile(r'(\\w)\\1{%d,}' % (max_repeats - 1))\n",
    "\n",
    "    # Función de sustitución para reducir repeticiones\n",
    "    def reduce_match(match):\n",
    "        return match.group(1) * max_repeats\n",
    "\n",
    "    # Aplicar la función de sustitución a la palabra\n",
    "    reduced_word = pattern.sub(reduce_match, word)\n",
    "\n",
    "    return reduced_word\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    #Convert all characters to lowercase from list of tokenized words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return str.join('',new_words)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Remove punctuation from text using regular expression\n",
    "    text_without_punctuation = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text_without_punctuation\n",
    "\n",
    "\n",
    "# Inicializar el objeto de la clase Inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "# Función para quitar numeros\n",
    "\n",
    "def remove_numbers(text):\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        # Verificar si la palabra contiene solo dígitos\n",
    "        if not re.match(r'^\\d+$', word):\n",
    "            words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def remove_stopwords_spacy(text):\n",
    "    \"\"\"\n",
    "    Stopwords de la libreria de spacy\n",
    "\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    palabras_sin_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(palabras_sin_stopwords)\n",
    "\n",
    "mis_stopwords = {\"a\", \"al\", \"algo\", \"algunas\", \"algunos\", \"ante\", \"antes\", \"como\", \"con\", \"contra\",\n",
    "        \"cual\", \"cualquier\", \"cualquieras\", \"cuales\", \"cualesquiera\", \"cuando\", \"de\", \"deja\",\n",
    "        \"dejais\", \"dejamos\", \"dejan\", \"dejar\", \"del\", \"demas\", \"demasiada\", \"demasiadas\",\n",
    "        \"demasiado\", \"demasiados\", \"dentro\", \"donde\", \"durante\", \"e\", \"el\", \"ella\", \"ellas\",\n",
    "        \"ellos\", \"en\", \"entre\", \"era\", \"erais\", \"eramos\", \"eran\", \"eras\", \"eres\", \"es\", \"esa\",\n",
    "        \"esas\", \"ese\", \"eso\", \"esos\", \"esta\", \"estaba\", \"estabais\", \"estábamos\", \"estaban\",\n",
    "        \"estabas\", \"estad\", \"estada\", \"estadas\", \"estado\", \"estados\", \"estamos\", \"estando\",\n",
    "        \"estar\", \"estara\", \"estaran\", \"estaras\", \"estare\", \"estareis\", \"estaremos\", \"estaría\",\n",
    "        \"estaríais\", \"estaríamos\", \"estarian\", \"estarias\", \"estas\", \"este\", \"esto\", \"estos\",\n",
    "        \"estoy\", \"estuve\", \"estuviera\", \"estuvierais\", \"estuvieran\", \"estuvieras\", \"estuvieron\",\n",
    "        \"estuviese\", \"estuvieseis\", \"estuviesemos\", \"estuviesen\", \"estuvieses\", \"estuvimos\",\n",
    "        \"estuviste\", \"estuvisteis\", \"estuvo\", \"está\", \"estabamos\", \"estais\", \"están\", \"estás\",\n",
    "        \"fue\", \"fuera\", \"fuerais\", \"fueran\", \"fueras\", \"fueron\", \"fuese\", \"fueseis\", \"fuésemos\",\n",
    "        \"fuesen\", \"fueses\", \"fui\", \"fuimos\", \"fuiste\", \"fuisteis\", \"ha\", \"habida\", \"habidas\",\n",
    "        \"habido\", \"habidos\", \"habiendo\", \"habremos\", \"habrá\", \"habrán\", \"habrás\", \"habré\",\n",
    "        \"habreis\", \"habremos\", \"habría\", \"habríais\", \"habríamos\", \"habrian\", \"habrias\", \"habeis\",\n",
    "        \"había\", \"habíais\", \"habíamos\", \"habían\", \"habías\", \"han\", \"has\", \"hasta\", \"hay\", \"haya\",\n",
    "        \"hayamos\", \"hayan\", \"hayas\", \"hayáis\", \"he\", \"hemos\", \"hube\", \"hubiera\", \"hubierais\",\n",
    "        \"hubieran\", \"hubieras\", \"hubieron\", \"hubiese\", \"hubieseis\", \"hubiesemos\", \"hubiesen\",\n",
    "        \"hubieses\", \"hubimos\", \"hubiste\", \"hubisteis\", \"hubo\", \"la\", \"las\", \"le\", \"les\", \"lo\", \"los\",\n",
    "        \"me\", \"mi\", \"mis\", \"mucho\", \"muchos\", \"muy\", \"mas\", \"mi\", \"mia\", \"mias\", \"mio\", \"mios\", \"nada\",\n",
    "        \"ni\", \"no\", \"nos\", \"nosotras\", \"nosotros\", \"nuestra\", \"nuestras\", \"nuestro\", \"nuestros\", \"o\",\n",
    "        \"os\", \"otra\", \"otras\", \"otro\", \"otros\", \"para\", \"pero\", \"poco\", \"por\", \"porque\", \"que\",\n",
    "        \"quien\", \"quienes\", \"qué\", \"se\", \"sea\", \"seamos\", \"sean\", \"seas\", \"sentid\", \"sentida\",\n",
    "        \"sentidas\", \"sentido\", \"sentidos\", \"seremos\", \"sera\", \"seran\", \"seras\", \"seré\", \"seréis\",\n",
    "        \"seremos\", \"seria\", \"seriais\", \"seríamos\", \"serían\", \"serías\", \"seáis\", \"siente\", \"sin\",\n",
    "        \"sintiendo\", \"sobre\", \"sois\", \"somos\", \"son\", \"soy\", \"su\", \"sus\", \"suya\", \"suyas\", \"suyo\",\n",
    "        \"suyos\", \"si\", \"tambien\", \"tanto\", \"te\", \"tendremos\", \"tendra\", \"tendran\", \"tendras\", \"tendre\",\n",
    "        \"tendréis\", \"tendríamos\", \"tendría\", \"tendríais\", \"tendríamos\", \"tendrían\", \"tendrías\",\n",
    "        \"tened\", \"tenemos\", \"tenga\", \"tengamos\", \"tengan\", \"tengas\", \"tengo\", \"tengais\", \"tenida\",\n",
    "        \"tenidas\", \"tenido\", \"tenidos\", \"teniendo\", \"teneis\", \"tenia\", \"teniais\", \"teniamos\", \"tenian\",\n",
    "        \"tenias\", \"ti\", \"tiene\", \"tienen\", \"tienes\", \"todo\", \"todos\", \"tu\", \"tus\", \"tuve\", \"tuviera\",\n",
    "        \"tuvierais\", \"tuvieran\", \"tuvieras\", \"tuvieron\", \"tuviese\", \"tuvieseis\", \"tuviésemos\", \"tuviesen\",\n",
    "        \"tuvieses\", \"tuvimos\", \"tuviste\", \"tuvisteis\", \"tuvo\", \"tuya\", \"tuyas\", \"tuyo\", \"tuyos\", \"tú\",\n",
    "        \"un\", \"una\", \"uno\", \"unos\", \"vosotras\", \"vosotros\", \"vuestra\", \"vuestras\", \"vuestro\", \"vuestros\",\n",
    "        \"y\", \"ya\", \"yo\", \"él\", \"eramos\"}\n",
    "\n",
    "# Función para quitar stopwords con spaCy, incluyendo las stopwords personalizadas\n",
    "def remove_stopwords_spacy(text):\n",
    "    for palabra in mis_stopwords:\n",
    "        nlp.Defaults.stop_words.add(palabra)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    palabras_sin_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "    # Verificar si la lista de palabras sin stopwords está vacía\n",
    "    if palabras_sin_stopwords:\n",
    "        return ' '.join(palabras_sin_stopwords)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def lemmatize_verbs(text):\n",
    "    lemmatized_tokens = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        try:\n",
    "            lemma = token.lemma_\n",
    "            if lemma:\n",
    "                lemmatized_tokens.append(lemma)\n",
    "            else:\n",
    "                lemmatized_tokens.append(token.text)  # Agregar la palabra original si no hay lema\n",
    "        except Exception as e:\n",
    "            print(f\"Error al lematizar el token '{token}': {e}\")\n",
    "            lemmatized_tokens.append(token.text)  # Agregar la palabra original en caso de error\n",
    "\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text \n",
    "\n",
    "def normalizador(df,columna):\n",
    "    df['text2'] = df[columna].apply(quitar_tildes)\n",
    "    df['text2'] = df['text2'].apply(remove_punctuation)\n",
    "    df['text2'] = df['text2'].apply(remove_special_characters)\n",
    "    df['text2'] = df['text2'].apply(remove_non_ascii)\n",
    "    df['text2'] = df['text2'].apply(reduce_repeated_characters)\n",
    "    df['text2'] = df['text2'].apply(to_lowercase)\n",
    "    df['text2'] = df['text2'].apply(remove_numbers)\n",
    "    df['text2'] = df['text2'].apply(remove_stopwords_spacy)\n",
    "    df['text2'] = df['text2'].apply(lemmatize_verbs)\n",
    "    return df['text2']\n",
    "\n",
    "\n",
    "#train_document = normalizador(train, 'document')\n",
    "#train_query =  normalizador(train, 'query')\n",
    "\n",
    "#test_document = normalizador(test_d, 'document')\n",
    "#test_query  = normalizador(test_q, 'query')\n",
    "\n",
    "train_document = pd.read_csv('train_document_vector.csv')\n",
    "train_query = pd.read_csv('train_query_vector.csv')\n",
    "train_label = pd.read_csv('train_label_vector.csv')\n",
    "\n",
    "test_document = pd.read_csv('test_document_vector.csv')\n",
    "test_query = pd.read_csv('test_query_vector.csv')\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Define el nombre del modelo y carga el modelo y el tokenizador\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_pad(df, text_column, tokenizer):\n",
    "    sequences = tokenizer(df[text_column].tolist(), return_tensors='pt', padding=True, truncation=True, max_length = 120)\n",
    "    return sequences['input_ids'], sequences['attention_mask'], sequences['token_type_ids']\n",
    "\n",
    "input_train_queries, attention_mask_train_queries, token_type_ids_train_queries = tokenize_and_pad(train_query, 'text2', tokenizer=tokenizer)\n",
    "input_train_documents, attention_mask_train_documents, token_type_ids_train_documents = tokenize_and_pad(train_document, 'text2', tokenizer=tokenizer)\n",
    "\n",
    "input_test_queries, attention_mask_test_queries, token_type_ids_test_queries  = tokenize_and_pad(test_query, 'text2', tokenizer=tokenizer)\n",
    "input_test_documents, attention_mask_test_documents, token_type_ids_test_documents = tokenize_and_pad(test_document, 'text2', tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 384)\n",
      "    (token_type_embeddings): Embedding(2, 384)\n",
      "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=384, out_features=4000, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "new_output_shape = 4000\n",
    "\n",
    "# Tamaño actual de la salida del modelo\n",
    "current_output_shape = model.pooler.dense.out_features\n",
    "\n",
    "# Verificar las diferencias en las dimensiones\n",
    "diff_shape = new_output_shape - model.pooler.dense.out_features\n",
    "\n",
    "# Agregar una capa lineal para ajustar las dimensiones\n",
    "model.pooler.dense = nn.Linear(384,current_output_shape + diff_shape)\n",
    "\n",
    "# Imprimir el modelo modificado\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\reto_ia\\ia\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de tensor2 después de la modificación: torch.Size([4000, 46080])\n",
      "Dimensiones de tensor2 después de la modificación: torch.Size([4000, 46080])\n",
      "Dimensiones de tensor2 después de la modificación: torch.Size([4000, 46080])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Linear \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Se crea el diccionario con las columnas input, attention_mask y token_type_ids para poder ingresarlos al modelo de hugging face.\n",
    "\n",
    "data_train_queries_dict = {\n",
    "    'input_ids': input_train_queries,\n",
    "    'attention_mask': attention_mask_train_queries,\n",
    "    'token_type_ids' : token_type_ids_train_queries\n",
    "}\n",
    "\n",
    "data_train_documents_dict = {\n",
    "    'input_ids': input_train_documents,\n",
    "    'attention_mask': attention_mask_train_documents,\n",
    "    'token_type_ids': token_type_ids_train_documents\n",
    "}\n",
    "\n",
    "\n",
    "model \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.set_grad_enabled(True): # calcular los gradiantes\n",
    "        queries_tokenized = data_train_documents_dict \n",
    "        documents_tokenized = data_train_queries_dict\n",
    "\n",
    "        queries_embeddings = model(**queries_tokenized)\n",
    "        documents_embeddings = model(**documents_tokenized)\n",
    "\n",
    "        queries_array =  queries_embeddings.last_hidden_state.view(4000, -1)\n",
    "        documents_array = documents_embeddings.last_hidden_state.view(4000, -1)\n",
    "\n",
    "\n",
    "        # Se verifica si queries_array es la misma dimension que documents_array \n",
    "        tensor1 = queries_array\n",
    "        tensor2 = documents_array \n",
    "        if tensor2.size() != tensor1.size():\n",
    "            # Calcular la diferencia en dimensiones\n",
    "            dif_dim = tensor1.size(1) - tensor2.size(1)\n",
    "            \n",
    "            # Verificar si tensor2 es menor que tensor1\n",
    "            if dif_dim > 0:\n",
    "                # Agregar ceros al final para hacer que tensor2 tenga las mismas dimensiones que tensor1\n",
    "                zeros_to_add = torch.zeros(tensor2.size(0), dif_dim)\n",
    "                tensor2 = torch.cat((tensor2, zeros_to_add), dim=1)\n",
    "            else:\n",
    "                print(\"El tensor 2 no es compatible con el tensor 1.\")\n",
    "        else:\n",
    "            print(\"Los tensores tienen las mismas dimensiones.\")\n",
    "\n",
    "        # Verificar las nuevas dimensiones de tensor2\n",
    "        print(\"Dimensiones de tensor2 después de la modificación:\", tensor2.size()) \n",
    "        # Realizar el calculo de la similitud\n",
    "        outputs = torch.cosine_similarity(tensor1, tensor2)\n",
    "\n",
    "        loss = criterion(outputs, torch.tensor(train['label']).float()) \n",
    "\n",
    "        # Realizar la retropropagación y la optimización\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Guardar tu modelo entrenado, si lo deseas\n",
    "torch.save(model.state_dict(), 'modelo_busqueda.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La funcion de perdida es:  tensor(3308.7229, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print('La funcion de perdida es: ', loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testeo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una funcion para obtener el tensor del texto y su codificacion al ser pasado al tokenizer\n",
    "\n",
    "def tokenize_and_pad_2(df, text_column, tokenizer):\n",
    "    # Tokenizar y generar secuencias\n",
    "    sequences = tokenizer(df[text_column].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=120)\n",
    "    \n",
    "    # Obtener los tokens de entrada, máscara de atención y tipo de tokens\n",
    "    input_ids = sequences['input_ids']\n",
    "    attention_mask = sequences['attention_mask']\n",
    "    token_type_ids = sequences['token_type_ids']\n",
    "    \n",
    "    # Decodificar los tokens para obtener las palabras originales\n",
    "    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "\n",
    "    # Almacenar el tensor asociado y la palabra asociada a ese tensor\n",
    "    token_tensor_pairs = []\n",
    "    for ids, decoded_text in zip(input_ids, decoded_texts):\n",
    "        token_tensor_pairs.append((ids, decoded_text))\n",
    "\n",
    "    tensor_list, word_list = zip(*token_tensor_pairs) \n",
    "    # Convertir las listas a tensores\n",
    "    token_tensors = torch.stack(tensor_list)\n",
    "    words = list(word_list)\n",
    "\n",
    "    return  token_tensors, words \n",
    "\n",
    "\n",
    "tensor_queries_tensor, tensor_queries_word = tokenize_and_pad_2(test_query, 'text2', tokenizer=tokenizer)\n",
    "tensor_documemt_tensor, tensor_document_word = tokenize_and_pad_2(test_document, 'text2', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean los diccionarios con las columnas input, attention_mask y token_type_id\n",
    "data_test_queries_dict = {\n",
    "    'input_ids': input_test_queries,\n",
    "    'attention_mask': attention_mask_test_queries,\n",
    "    'token_type_ids' : token_type_ids_test_queries\n",
    "    # 'word': tensor_queries_word\n",
    "}\n",
    "\n",
    "data_test_documents_dict = {\n",
    "    'input_ids': input_test_documents,\n",
    "    'attention_mask': attention_mask_test_documents,\n",
    "    'token_type_ids': token_type_ids_test_documents\n",
    "    #'word': tensor_document_word\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=4000, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se carga el modelo guardado anteriormente y se pone en modo evaluacion\n",
    "model.load_state_dict(torch.load('modelo_busqueda.pth'))\n",
    "model.eval()  # Coloca el modelo en modo de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de tensor2_ después de la modificación: torch.Size([100, 35712])\n"
     ]
    }
   ],
   "source": [
    "# Estae codigo deja en las dimensiones del tensor1_ y le reduce la dimensionalidad al tensor2_ para poder calcular la similitud\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# Pasar los datos de prueba a través del modelo cargado\n",
    "with torch.no_grad():  \n",
    "    # Deshabilitar el cálculo de gradientes durante la inferencia\n",
    "    test_documents_embedding = model(**data_test_documents_dict).last_hidden_state.view(100, -1)\n",
    "    test_queries_embedding = model(**data_test_queries_dict).last_hidden_state.view(100, -1)\n",
    "\n",
    "    tensor1_ = test_queries_embedding \n",
    "    tensor2_ = test_documents_embedding \n",
    "\n",
    "    if tensor2_.size() != tensor1_.size():\n",
    "        # Calcular la diferencia en dimensiones\n",
    "        dif_dim_ = abs(tensor1_.size(1) - tensor2_.size(1))\n",
    "        \n",
    "        # Verificar cuál tensor es mayor\n",
    "        if tensor2_.size(1) > tensor1_.size(1):\n",
    "            # Reducir la dimensión de tensor2_ para que tenga las mismas dimensiones que tensor1_\n",
    "            tensor2_ = tensor2_[:, :tensor1_.size(1)]\n",
    "        else:\n",
    "            # Agregar ceros al tensor 2 para hacer que tenga las mismas dimensiones que tensor 1\n",
    "            zeros_to_add_ = torch.zeros(tensor2_.size(0), dif_dim_)\n",
    "            tensor2_ = torch.cat((tensor2_, zeros_to_add_), dim=1)\n",
    "    else:\n",
    "        print(\"Los tensores tienen las mismas dimensiones.\")\n",
    "\n",
    "    # Verificar las nuevas dimensiones de tensor2_\n",
    "    print(\"Dimensiones de tensor2_ después de la modificación:\", tensor2_.size()) \n",
    "    \n",
    "    consulta_y_documentos_similares_id = []\n",
    "    consulta_y_documentos_similares = []\n",
    "\n",
    "    for i in range(tensor1_.size(0)):\n",
    "    # Calcular la similitud coseno entre la consulta y cada documento\n",
    "        similarities_ = util.cos_sim(tensor1_[i], tensor2_)\n",
    "        word_query = tensor_queries_word[i]\n",
    "        word_query_id = test_q['query_id'][i]\n",
    "        # Obtener los índices de los documentos más similares (en orden descendente)\n",
    "        top_document_indices_ = similarities_.argsort(descending=True)[0][:10] \n",
    "\n",
    "        documentos_similares_id = []\n",
    "        documentos_similares = []\n",
    "        for index in range(10):\n",
    "            indice = top_document_indices_.tolist()[index]\n",
    "            documento_similar_id = test_d['document_id'][indice] \n",
    "            documento_similar = tensor_document_word[indice]\n",
    "            documentos_similares_id.append(documento_similar_id) \n",
    "            documentos_similares.append(documento_similar) \n",
    "\n",
    "\n",
    "        consulta_y_documentos_similares_id.append({'query_id': word_query_id, 'document_id': documentos_similares_id})\n",
    "        consulta_y_documentos_similares.append({'query': word_query, 'document_id': documentos_similares})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"KWjLrafk\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"S26n47qu\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"Jj7Rmaeu\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"XxxdpKqh\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"Ix7Dq5ey\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"YYYk5ii6\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"q61QVoYB\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"ajy8a1ci\": [\n",
      "        \"Dccyxaec\",\n",
      "        \"tgzu0Xnp\",\n",
      "        \"qJFN3ljy\",\n",
      "        \"k7eoMn2z\",\n",
      "        \"gv4dhAKa\",\n",
      "        \"7oOCtDEC\",\n",
      "        \"xAbD8BdJ\",\n",
      "        \"4IRLvDf0\",\n",
      "        \"418e6axX\",\n",
      "        \"Qs2yt0jz\"\n",
      "    ],\n",
      "    \"1DLF7oIg\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"QRXypyjo\": [\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"4RQdwYz4\"\n",
      "    ],\n",
      "    \"8bbMJnaw\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"CvosEpdI\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"xLuAFxw6\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"Gw5EIYiX\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"GowjI52g\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"bIlSbUHr\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"ydvQLaEc\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"0rrT0XI5\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"DEnkR7RH\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"34pM6WBS\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"2tvxhcos\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"EhBiK7eP\": [\n",
      "        \"VZ8Ui042\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"3nei7MO8\",\n",
      "        \"34pM6WBS\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"gD53zFDC\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"t46UH9zP\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"3nei7MO8\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"1VSSgI2N\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"DRKU1EoT\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"Q775fwQS\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"P2IzhUKb\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"dV0pRAyp\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"N7rxeROB\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"xhUSuRzQ\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"t2oCIeoE\"\n",
      "    ],\n",
      "    \"dWpZBq7e\": [\n",
      "        \"t2oCIeoE\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"wl6h31Dm\",\n",
      "        \"t7MFggR0\",\n",
      "        \"pCp2q04C\",\n",
      "        \"6ZTWDGPA\"\n",
      "    ],\n",
      "    \"qEyQL9GO\": [\n",
      "        \"UphLchV1\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"3nei7MO8\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"NLSofyXS\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"3nei7MO8\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"y4IX1RON\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"ZkjlnYeQ\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"U7YLpyWe\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"R5twfNxQ\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"4Rnl8dGm\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"1MbVEVsM\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"ZYrxqSzQ\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"agiJqE9S\": [\n",
      "        \"4IRLvDf0\",\n",
      "        \"xgiKBidw\",\n",
      "        \"9l7XPy1G\",\n",
      "        \"vgQzgKYE\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"Ym4PcQkJ\",\n",
      "        \"4RhsYS3e\",\n",
      "        \"gv4dhAKa\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"lWHJe81B\"\n",
      "    ],\n",
      "    \"n2ytC8zF\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"qFvogiiX\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"fIPxdpEw\": [\n",
      "        \"wl6h31Dm\",\n",
      "        \"UphLchV1\",\n",
      "        \"qSRq7DGj\",\n",
      "        \"t2oCIeoE\",\n",
      "        \"GnpydQjA\",\n",
      "        \"N7IrG10d\",\n",
      "        \"4RQdwYz4\",\n",
      "        \"T5CUZk3m\",\n",
      "        \"3nei7MO8\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"9748TH4K\": [\n",
      "        \"olvCZ9Iy\",\n",
      "        \"1iUaZn4a\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"4nggpl7e\",\n",
      "        \"9TvbhKcb\",\n",
      "        \"GnpydQjA\",\n",
      "        \"UphLchV1\",\n",
      "        \"lJ3rvviJ\",\n",
      "        \"f52tIltf\",\n",
      "        \"CX5Xq7J9\"\n",
      "    ],\n",
      "    \"rCYCMEZD\": [\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"UphLchV1\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"XmQ3tYO7\": [\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"UphLchV1\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"vgQzgKYE\"\n",
      "    ],\n",
      "    \"1FQIwCY7\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"wL8IZezC\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"EtaW8pQG\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"3nei7MO8\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"PqD7AaXB\": [\n",
      "        \"GnpydQjA\",\n",
      "        \"9TvbhKcb\",\n",
      "        \"1iUaZn4a\",\n",
      "        \"lJ3rvviJ\",\n",
      "        \"ozK0nHFp\",\n",
      "        \"4nggpl7e\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"vgQzgKYE\",\n",
      "        \"YEwx5F24\",\n",
      "        \"t2oCIeoE\"\n",
      "    ],\n",
      "    \"LwLt9m19\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"1Ea3JCJc\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"Ppf54X2R\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"4RQdwYz4\",\n",
      "        \"t2oCIeoE\"\n",
      "    ],\n",
      "    \"C7Nai0lI\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"uB1z7ypx\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"yLJX9Ise\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"MSKJOoQM\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"4RQdwYz4\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"WdGqjJw6\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"rLnArvil\": [\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"34pM6WBS\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"ZZWxYAyq\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"s4QjdYgt\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"vQzmfAeu\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"PcpTKE5t\": [\n",
      "        \"UphLchV1\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"0HPrjU8I\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"jOu1BFKG\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"9PEhSZLP\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"gD53zFDC\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"TFY81VIq\": [\n",
      "        \"hXnXhyAB\",\n",
      "        \"SkLB8AlC\",\n",
      "        \"OVXh6MOB\",\n",
      "        \"jzH4BWxe\",\n",
      "        \"efGL2lMD\",\n",
      "        \"t2oCIeoE\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"F2QFOGNI\",\n",
      "        \"4nggpl7e\"\n",
      "    ],\n",
      "    \"pSUOdBJt\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"d5ic43Gp\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"SEkwgiyA\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"aicAAce7\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"sgoKXgTo\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"T5CUZk3m\",\n",
      "        \"3nei7MO8\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"0OOemdJR\": [\n",
      "        \"EfHVfhi9\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"t7MFggR0\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"pCp2q04C\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"vgQzgKYE\",\n",
      "        \"1iUaZn4a\"\n",
      "    ],\n",
      "    \"wPo9dsCk\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"BxuVSl4s\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"OfMQrntb\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"ULQcn0Cs\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"8pa8ozzl\": [\n",
      "        \"CiPeiP3k\",\n",
      "        \"9TvbhKcb\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"CX5Xq7J9\",\n",
      "        \"vgQzgKYE\",\n",
      "        \"f7fP3ubj\",\n",
      "        \"lJ3rvviJ\",\n",
      "        \"OVXh6MOB\",\n",
      "        \"1iUaZn4a\",\n",
      "        \"KaYiSh7g\"\n",
      "    ],\n",
      "    \"A9BCPXRJ\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"Ims3LlTp\": [\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\"\n",
      "    ],\n",
      "    \"ggekQiB2\": [\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"T5CUZk3m\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"FqIPSPdW\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"Sze1WMt7\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"GxDTj6q8\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\",\n",
      "        \"4RQdwYz4\"\n",
      "    ],\n",
      "    \"wW3XZpOW\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"3nei7MO8\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"1Hobxbuu\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"WvpwjGdB\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"eRpqs42o\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\"\n",
      "    ],\n",
      "    \"w4FZt06O\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"uOWiiO5Q\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"EfHVfhi9\",\n",
      "        \"olvCZ9Iy\"\n",
      "    ],\n",
      "    \"G6bVIa09\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"QTuRn760\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"wbNEUsXr\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"34pM6WBS\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\",\n",
      "        \"gD53zFDC\"\n",
      "    ],\n",
      "    \"rl9Eawyy\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"t7MFggR0\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"34pM6WBS\",\n",
      "        \"gD53zFDC\",\n",
      "        \"3nei7MO8\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"mkMf8cHV\": [\n",
      "        \"UphLchV1\",\n",
      "        \"6ZTWDGPA\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\",\n",
      "        \"pCp2q04C\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"D60EEXzb\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"34pM6WBS\",\n",
      "        \"4RQdwYz4\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"3nei7MO8\",\n",
      "        \"EfHVfhi9\"\n",
      "    ],\n",
      "    \"yo0TpLTg\": [\n",
      "        \"f52tIltf\",\n",
      "        \"Lqv0xS6x\",\n",
      "        \"vgQzgKYE\",\n",
      "        \"GnpydQjA\",\n",
      "        \"jzH4BWxe\",\n",
      "        \"CX5Xq7J9\",\n",
      "        \"vdejhJop\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"efGL2lMD\",\n",
      "        \"t7MFggR0\"\n",
      "    ],\n",
      "    \"ijuR8Fdr\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"t7MFggR0\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"pCp2q04C\",\n",
      "        \"gD53zFDC\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"VZ8Ui042\"\n",
      "    ],\n",
      "    \"tGAHXFFn\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"34pM6WBS\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"pCp2q04C\",\n",
      "        \"3nei7MO8\",\n",
      "        \"gD53zFDC\",\n",
      "        \"4RQdwYz4\"\n",
      "    ],\n",
      "    \"FvS8x6dv\": [\n",
      "        \"6ZTWDGPA\",\n",
      "        \"UphLchV1\",\n",
      "        \"t7MFggR0\",\n",
      "        \"kPEDk3X6\",\n",
      "        \"53Q5nZCn\",\n",
      "        \"34pM6WBS\",\n",
      "        \"4RQdwYz4\",\n",
      "        \"pCp2q04C\",\n",
      "        \"olvCZ9Iy\",\n",
      "        \"gD53zFDC\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Crear un diccionario para almacenar los resultados\n",
    "results_dict_id = {}\n",
    "\n",
    "# Procesar cada consulta y encontrar los documentos más similares\n",
    "for i in range(len(consulta_y_documentos_similares_id)):\n",
    "    # Obtener el word_query_id correspondiente a esta consulta\n",
    "    word_query_id = consulta_y_documentos_similares_id[i]['query_id']\n",
    "    # Agregar los IDs de los documentos al diccionario\n",
    "    results_dict_id[word_query_id] = consulta_y_documentos_similares_id[i]['document_id']\n",
    "\n",
    "# Convertir el diccionario a formato JSON\n",
    "import json\n",
    "json_results_id = json.dumps(results_dict_id, indent=4)\n",
    "\n",
    "# Imprimir los resultados JSON\n",
    "print(json_results_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardamos los resultados\n",
    "ruta_archivo = 'resultado_semantic_search.json'\n",
    "\n",
    "# Guarda los datos en el archivo JSON\n",
    "with open(ruta_archivo, 'w') as archivo:\n",
    "    json.dump(results_dict_id , archivo) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
